{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: EDA v√† X·ª≠ l√Ω D·ªØ li·ªáu - D·ª± ƒëo√°n Gi√° Nh√†\n",
    "\n",
    "## M·ª•c ti√™u\n",
    "- Ph√¢n t√≠ch kh√°m ph√° d·ªØ li·ªáu (EDA)\n",
    "- X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu v√† ngo·∫°i lai\n",
    "- Chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh prediction intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# C·∫•u h√¨nh hi·ªÉn th·ªã\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫£i d·ªØ li·ªáu t·ª´ Kaggle\n",
    "# L∆∞u √Ω: B·∫°n c·∫ßn download d·ªØ li·ªáu t·ª´ cu·ªôc thi Kaggle tr∆∞·ªõc\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc t·∫≠p train: {train_df.shape}\")\n",
    "print(f\"K√≠ch th∆∞·ªõc t·∫≠p test: {test_df.shape}\")\n",
    "print(f\"\\nC·ªôt trong train: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ph√¢n t√≠ch Th·ªëng k√™ M√¥ t·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th√¥ng tin c∆° b·∫£n v·ªÅ dataset\n",
    "def basic_info_analysis(df, name=\"Dataset\"):\n",
    "    print(f\"=== TH√îNG TIN C∆† B·∫¢N - {name} ===\")\n",
    "    print(f\"K√≠ch th∆∞·ªõc: {df.shape}\")\n",
    "    print(f\"Ki·ªÉu d·ªØ li·ªáu:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    print(f\"\\nD·ªØ li·ªáu thi·∫øu:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing,\n",
    "        'Missing_Percent': missing_percent\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False))\n",
    "    return missing_df\n",
    "\n",
    "# Ph√¢n t√≠ch train set\n",
    "train_missing = basic_info_analysis(train_df, \"TRAIN SET\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "test_missing = basic_info_analysis(test_df, \"TEST SET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th·ªëng k√™ m√¥ t·∫£ cho bi·∫øn m·ª•c ti√™u (SalePrice)\n",
    "if 'SalePrice' in train_df.columns:\n",
    "    print(\"=== TH·ªêNG K√ä SALEPRICE ===\")\n",
    "    print(train_df['SalePrice'].describe())\n",
    "    print(f\"\\nSkewness: {train_df['SalePrice'].skew():.3f}\")\n",
    "    print(f\"Kurtosis: {train_df['SalePrice'].kurtosis():.3f}\")\n",
    "    \n",
    "    # Ph√¢n t√≠ch outliers\n",
    "    Q1 = train_df['SalePrice'].quantile(0.25)\n",
    "    Q3 = train_df['SalePrice'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = train_df[(train_df['SalePrice'] < lower_bound) | (train_df['SalePrice'] > upper_bound)]\n",
    "    print(f\"\\nS·ªë l∆∞·ª£ng outliers: {len(outliers)} ({len(outliers)/len(train_df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ph√¢n t√≠ch Ph√¢n ph·ªëi Gi√° Nh√†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch ph√¢n ph·ªëi SalePrice\n",
    "def analyze_price_distribution(df):\n",
    "    if 'SalePrice' not in df.columns:\n",
    "        print(\"Kh√¥ng c√≥ c·ªôt SalePrice trong dataset\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # 1. Histogram g·ªëc\n",
    "    axes[0,0].hist(df['SalePrice'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].set_title('Ph√¢n ph·ªëi Gi√° Nh√† (G·ªëc)', fontsize=14, fontweight='bold')\n",
    "    axes[0,0].set_xlabel('Gi√° nh√† ($)')\n",
    "    axes[0,0].set_ylabel('T·∫ßn su·∫•t')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Log transformation\n",
    "    log_price = np.log(df['SalePrice'])\n",
    "    axes[0,1].hist(log_price, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0,1].set_title('Ph√¢n ph·ªëi Log(Gi√°)', fontsize=14, fontweight='bold')\n",
    "    axes[0,1].set_xlabel('Log(Gi√° nh√†)')\n",
    "    axes[0,1].set_ylabel('T·∫ßn su·∫•t')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Box plot\n",
    "    axes[0,2].boxplot(df['SalePrice'])\n",
    "    axes[0,2].set_title('Box Plot Gi√° Nh√†', fontsize=14, fontweight='bold')\n",
    "    axes[0,2].set_ylabel('Gi√° nh√† ($)')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Q-Q plot g·ªëc\n",
    "    stats.probplot(df['SalePrice'], dist=\"norm\", plot=axes[1,0])\n",
    "    axes[1,0].set_title('Q-Q Plot (G·ªëc)', fontsize=14, fontweight='bold')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Q-Q plot log\n",
    "    stats.probplot(log_price, dist=\"norm\", plot=axes[1,1])\n",
    "    axes[1,1].set_title('Q-Q Plot (Log)', fontsize=14, fontweight='bold')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Density plot\n",
    "    axes[1,2].plot(df['SalePrice'], stats.gaussian_kde(df['SalePrice'])(df['SalePrice']), 'b-', alpha=0.7)\n",
    "    axes[1,2].set_title('Density Plot', fontsize=14, fontweight='bold')\n",
    "    axes[1,2].set_xlabel('Gi√° nh√† ($)')\n",
    "    axes[1,2].set_ylabel('Density')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Th·ªëng k√™ so s√°nh\n",
    "    print(\"=== SO S√ÅNH PH√ÇN PH·ªêI ===\")\n",
    "    print(f\"Skewness g·ªëc: {df['SalePrice'].skew():.3f}\")\n",
    "    print(f\"Skewness log: {log_price.skew():.3f}\")\n",
    "    print(f\"Kurtosis g·ªëc: {df['SalePrice'].kurtosis():.3f}\")\n",
    "    print(f\"Kurtosis log: {log_price.kurtosis():.3f}\")\n",
    "\n",
    "analyze_price_distribution(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ph√¢n t√≠ch T∆∞∆°ng quan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch t∆∞∆°ng quan v·ªõi SalePrice\n",
    "def correlation_analysis(df):\n",
    "    if 'SalePrice' not in df.columns:\n",
    "        print(\"Kh√¥ng c√≥ c·ªôt SalePrice ƒë·ªÉ ph√¢n t√≠ch t∆∞∆°ng quan\")\n",
    "        return\n",
    "    \n",
    "    # Ch·ªâ l·∫•y c√°c c·ªôt s·ªë\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # V·∫Ω heatmap\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, fmt='.2f')\n",
    "    plt.title('Ma tr·∫≠n T∆∞∆°ng quan', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Top correlations v·ªõi SalePrice\n",
    "    price_corr = corr_matrix['SalePrice'].abs().sort_values(ascending=False)\n",
    "    print(\"=== TOP 15 FEATURES T∆Ø∆†NG QUAN CAO NH·∫§T V·ªöI SALEPRICE ===\")\n",
    "    for i, (feature, corr) in enumerate(price_corr.head(16).items()):\n",
    "        if feature != 'SalePrice':\n",
    "            print(f\"{i:2d}. {feature:20s}: {corr:.4f}\")\n",
    "    \n",
    "    return price_corr.head(16)\n",
    "\n",
    "top_corr = correlation_analysis(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V·∫Ω scatter plots cho top features\n",
    "def plot_top_correlations(df, top_features, n_plots=6):\n",
    "    if 'SalePrice' not in df.columns:\n",
    "        return\n",
    "    \n",
    "    # L·∫•y top features (lo·∫°i tr·ª´ SalePrice)\n",
    "    features_to_plot = [f for f in top_features.index[:n_plots+1] if f != 'SalePrice'][:n_plots]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(features_to_plot):\n",
    "        if feature in df.columns:\n",
    "            axes[i].scatter(df[feature], df['SalePrice'], alpha=0.6, s=30)\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('SalePrice')\n",
    "            axes[i].set_title(f'{feature} vs SalePrice\\n(Corr: {top_features[feature]:.3f})')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Th√™m trend line\n",
    "            z = np.polyfit(df[feature].fillna(df[feature].median()), df['SalePrice'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[i].plot(df[feature], p(df[feature]), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top_correlations(train_df, top_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. X·ª≠ l√Ω D·ªØ li·ªáu Thi·∫øu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu\n",
    "def handle_missing_values(train_df, test_df):\n",
    "    # K·∫øt h·ª£p train v√† test ƒë·ªÉ x·ª≠ l√Ω ƒë·ªìng nh·∫•t\n",
    "    all_data = pd.concat([train_df, test_df], sort=False, ignore_index=True)\n",
    "    \n",
    "    # Ph√¢n lo·∫°i c√°c c·ªôt theo ki·ªÉu d·ªØ li·ªáu\n",
    "    numeric_cols = all_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = all_data.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"S·ªë c·ªôt s·ªë: {len(numeric_cols)}\")\n",
    "    print(f\"S·ªë c·ªôt categorical: {len(categorical_cols)}\")\n",
    "    \n",
    "    # X·ª≠ l√Ω missing values cho numeric columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'SalePrice':  # Kh√¥ng x·ª≠ l√Ω target variable\n",
    "            missing_count = all_data[col].isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                # D√πng median ƒë·ªÉ fill missing values\n",
    "                median_val = all_data[col].median()\n",
    "                all_data[col].fillna(median_val, inplace=True)\n",
    "                print(f\"Filled {missing_count} missing values in {col} with median: {median_val}\")\n",
    "    \n",
    "    # X·ª≠ l√Ω missing values cho categorical columns\n",
    "    for col in categorical_cols:\n",
    "        missing_count = all_data[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            # D√πng mode ƒë·ªÉ fill missing values\n",
    "            mode_val = all_data[col].mode().iloc[0] if len(all_data[col].mode()) > 0 else 'Unknown'\n",
    "            all_data[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"Filled {missing_count} missing values in {col} with mode: {mode_val}\")\n",
    "    \n",
    "    # T√°ch l·∫°i train v√† test\n",
    "    train_processed = all_data[:len(train_df)].copy()\n",
    "    test_processed = all_data[len(train_df):].copy()\n",
    "    \n",
    "    return train_processed, test_processed\n",
    "\n",
    "train_processed, test_processed = handle_missing_values(train_df, test_df)\n",
    "print(f\"\\nSau x·ª≠ l√Ω - Train missing values: {train_processed.isnull().sum().sum()}\")\n",
    "print(f\"Sau x·ª≠ l√Ω - Test missing values: {test_processed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def feature_engineering(train_df, test_df):\n",
    "    train_fe = train_df.copy()\n",
    "    test_fe = test_df.copy()\n",
    "    \n",
    "    # T·∫°o features m·ªõi cho c·∫£ train v√† test\n",
    "    for df in [train_fe, test_fe]:\n",
    "        # 1. Total area features\n",
    "        if all(col in df.columns for col in ['1stFlrSF', '2ndFlrSF']):\n",
    "            df['TotalSF'] = df['1stFlrSF'] + df['2ndFlrSF']\n",
    "        \n",
    "        if all(col in df.columns for col in ['TotalBsmtSF', 'GrLivArea']):\n",
    "            df['TotalArea'] = df['TotalBsmtSF'] + df['GrLivArea']\n",
    "        \n",
    "        # 2. Age features\n",
    "        if 'YearBuilt' in df.columns:\n",
    "            df['HouseAge'] = 2024 - df['YearBuilt']\n",
    "        \n",
    "        if 'YearRemodAdd' in df.columns:\n",
    "            df['RemodAge'] = 2024 - df['YearRemodAdd']\n",
    "        \n",
    "        # 3. Bathroom features\n",
    "        if all(col in df.columns for col in ['FullBath', 'HalfBath']):\n",
    "            df['TotalBath'] = df['FullBath'] + 0.5 * df['HalfBath']\n",
    "        \n",
    "        # 4. Quality features\n",
    "        if all(col in df.columns for col in ['OverallQual', 'OverallCond']):\n",
    "            df['QualCondProduct'] = df['OverallQual'] * df['OverallCond']\n",
    "        \n",
    "        # 5. Garage features\n",
    "        if 'GarageArea' in df.columns and 'GarageCars' in df.columns:\n",
    "            df['GarageAreaPerCar'] = df['GarageArea'] / (df['GarageCars'] + 1)  # +1 ƒë·ªÉ tr√°nh chia 0\n",
    "    \n",
    "    print(\"=== FEATURES M·ªöI ƒê√É T·∫†O ===\")\n",
    "    new_features = ['TotalSF', 'TotalArea', 'HouseAge', 'RemodAge', 'TotalBath', 'QualCondProduct', 'GarageAreaPerCar']\n",
    "    for feature in new_features:\n",
    "        if feature in train_fe.columns:\n",
    "            print(f\"‚úì {feature}\")\n",
    "    \n",
    "    return train_fe, test_fe\n",
    "\n",
    "train_fe, test_fe = feature_engineering(train_processed, test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. X·ª≠ l√Ω Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√°t hi·ªán v√† x·ª≠ l√Ω outliers\n",
    "def detect_outliers(df, columns, method='IQR'):\n",
    "    outliers_dict = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns and df[col].dtype in ['int64', 'float64']:\n",
    "            if method == 'IQR':\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
    "            elif method == 'Z-score':\n",
    "                z_scores = np.abs(stats.zscore(df[col]))\n",
    "                outliers = df[z_scores > 3].index\n",
    "            \n",
    "            outliers_dict[col] = outliers\n",
    "    \n",
    "    return outliers_dict\n",
    "\n",
    "# Ph√°t hi·ªán outliers trong SalePrice v√† top features\n",
    "if 'SalePrice' in train_fe.columns:\n",
    "    important_cols = ['SalePrice'] + [col for col in top_corr.index[:6] if col != 'SalePrice' and col in train_fe.columns]\n",
    "    outliers_dict = detect_outliers(train_fe, important_cols)\n",
    "    \n",
    "    print(\"=== OUTLIERS PH√ÅT HI·ªÜN ===\")\n",
    "    for col, outliers in outliers_dict.items():\n",
    "        print(f\"{col}: {len(outliers)} outliers ({len(outliers)/len(train_fe)*100:.2f}%)\")\n",
    "    \n",
    "    # Visualize outliers\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(important_cols[:6]):\n",
    "        if col in train_fe.columns:\n",
    "            axes[i].boxplot(train_fe[col].dropna())\n",
    "            axes[i].set_title(f'Box Plot - {col}')\n",
    "            axes[i].set_ylabel(col)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "def encode_categorical_features(train_df, test_df):\n",
    "    train_encoded = train_df.copy()\n",
    "    test_encoded = test_df.copy()\n",
    "    \n",
    "    # L·∫•y c√°c c·ªôt categorical\n",
    "    categorical_cols = train_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"=== ENCODING {len(categorical_cols)} CATEGORICAL FEATURES ===\")\n",
    "    \n",
    "    # K·∫øt h·ª£p train v√† test ƒë·ªÉ ƒë·∫£m b·∫£o consistent encoding\n",
    "    all_data = pd.concat([train_encoded, test_encoded], sort=False)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        # Ki·ªÉm tra s·ªë l∆∞·ª£ng unique values\n",
    "        unique_count = all_data[col].nunique()\n",
    "        print(f\"{col}: {unique_count} unique values\")\n",
    "        \n",
    "        if unique_count <= 10:  # One-hot encoding cho √≠t categories\n",
    "            dummies = pd.get_dummies(all_data[col], prefix=col, dummy_na=True)\n",
    "            all_data = pd.concat([all_data.drop(col, axis=1), dummies], axis=1)\n",
    "        else:  # Label encoding cho nhi·ªÅu categories\n",
    "            le = LabelEncoder()\n",
    "            all_data[col] = le.fit_transform(all_data[col].astype(str))\n",
    "    \n",
    "    # T√°ch l·∫°i train v√† test\n",
    "    train_encoded = all_data[:len(train_df)].copy()\n",
    "    test_encoded = all_data[len(train_df):].copy()\n",
    "    \n",
    "    print(f\"\\nSau encoding:\")\n",
    "    print(f\"Train shape: {train_encoded.shape}\")\n",
    "    print(f\"Test shape: {test_encoded.shape}\")\n",
    "    \n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "train_encoded, test_encoded = encode_categorical_features(train_fe, test_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scaling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "def scale_features(train_df, test_df, target_col='SalePrice', method='robust'):\n",
    "    # T√°ch features v√† target\n",
    "    if target_col in train_df.columns:\n",
    "        X_train = train_df.drop(target_col, axis=1)\n",
    "        y_train = train_df[target_col]\n",
    "    else:\n",
    "        X_train = train_df.copy()\n",
    "        y_train = None\n",
    "    X_test = test_df.copy()\n",
    "    \n",
    "    # Ch·ªâ scale numeric columns\n",
    "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if method == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    # Fit tr√™n train, transform c·∫£ train v√† test\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "    X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "    \n",
    "    print(f\"=== SCALING COMPLETED USING {method.upper()} SCALER ===\")\n",
    "    print(f\"Scaled {len(numeric_cols)} numeric features\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, scaler\n",
    "\n",
    "X_train_scaled, X_test_scaled, y_train, scaler = scale_features(train_encoded, test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. L∆∞u D·ªØ li·ªáu ƒê√£ X·ª≠ l√Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω\n",
    "def save_processed_data(X_train, X_test, y_train, scaler):\n",
    "    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "    import os\n",
    "    os.makedirs('processed_data', exist_ok=True)\n",
    "    \n",
    "    # L∆∞u features\n",
    "    X_train.to_csv('processed_data/X_train_processed.csv', index=False)\n",
    "    X_test.to_csv('processed_data/X_test_processed.csv', index=False)\n",
    "    \n",
    "    # L∆∞u target\n",
    "    if y_train is not None:\n",
    "        pd.DataFrame({'SalePrice': y_train}).to_csv('processed_data/y_train.csv', index=False)\n",
    "    \n",
    "    # L∆∞u scaler\n",
    "    import joblib\n",
    "    joblib.dump(scaler, 'processed_data/scaler.pkl')\n",
    "    \n",
    "    print(\"=== D·ªÆ LI·ªÜU ƒê√É ƒê∆Ø·ª¢C LUU ===\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    if y_train is not None:\n",
    "        print(f\"y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "save_processed_data(X_train_scaled, X_test_scaled, y_train, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. T√≥m t·∫Øt EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≥m t·∫Øt k·∫øt qu·∫£ EDA\n",
    "def eda_summary():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"                    T√ìM T·∫ÆT EDA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nüìä TH·ªêNG K√ä DATASET:\")\n",
    "    print(f\"   ‚Ä¢ Train samples: {len(train_df)}\")\n",
    "    print(f\"   ‚Ä¢ Test samples: {len(test_df)}\")\n",
    "    print(f\"   ‚Ä¢ Features sau x·ª≠ l√Ω: {X_train_scaled.shape[1]}\")\n",
    "    \n",
    "    if 'SalePrice' in train_df.columns:\n",
    "        print(f\"\\nüí∞ PH√ÇN T√çCH GI√Å NH√Ä:\")\n",
    "        print(f\"   ‚Ä¢ Gi√° trung b√¨nh: ${train_df['SalePrice'].mean():,.0f}\")\n",
    "        print(f\"   ‚Ä¢ Gi√° median: ${train_df['SalePrice'].median():,.0f}\")\n",
    "        print(f\"   ‚Ä¢ Kho·∫£ng gi√°: ${train_df['SalePrice'].min():,.0f} - ${train_df['SalePrice'].max():,.0f}\")\n",
    "        print(f\"   ‚Ä¢ Skewness: {train_df['SalePrice'].skew():.3f}\")\n",
    "    \n",
    "    print(f\"\\nüîß X·ª¨ L√ù D·ªÆ LI·ªÜU:\")\n",
    "    print(f\"   ‚Ä¢ Missing values: ƒê√£ x·ª≠ l√Ω\")\n",
    "    print(f\"   ‚Ä¢ Categorical encoding: ƒê√£ th·ª±c hi·ªán\")\n",
    "    print(f\"   ‚Ä¢ Feature scaling: RobustScaler\")\n",
    "    print(f\"   ‚Ä¢ Feature engineering: ƒê√£ t·∫°o features m·ªõi\")\n",
    "    \n",
    "    if 'top_corr' in globals():\n",
    "        print(f\"\\nüìà TOP 5 FEATURES QUAN TR·ªåNG:\")\n",
    "        for i, (feature, corr) in enumerate(list(top_corr.items())[1:6]):\n",
    "            print(f\"   {i+1}. {feature}: {corr:.3f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ CHU·∫®N B·ªä CHO MODELING:\")\n",
    "    print(f\"   ‚Ä¢ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c 'processed_data'\")\n",
    "    print(f\"   ‚Ä¢ S·∫µn s√†ng cho b∆∞·ªõc hu·∫•n luy·ªán m√¥ h√¨nh\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "eda_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
